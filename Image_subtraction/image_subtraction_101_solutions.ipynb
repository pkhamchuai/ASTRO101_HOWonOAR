{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414608c2",
   "metadata": {},
   "source": [
    "# Image subtraction and transient detection (using STDPipe package)\n",
    "\n",
    "**Lecturer:** Sergey Karpov<br>\n",
    "**Jupyter Notebook author:** Sergey Karpov\n",
    "\n",
    "## Objective\n",
    "\n",
    "Learn how to perform image subtraction and discover astronomical transients in the images\n",
    "\n",
    "## Key steps\n",
    "\n",
    "- Pre-process and mask bad regions in the image\n",
    "- Detect stars in the image and use them for astrometric and photometric calibration\n",
    "- Get the reference image from public data archives\n",
    "- Perform image subtraction\n",
    "- Find the transients in the difference image\n",
    "\n",
    "As most of lower-level steps are already covered in previous tutorials, we will use higher-level routines from *STDPipe* package for handling them\n",
    "\n",
    "## Required Python modules\n",
    "\n",
    "- astropy\n",
    "- numpy\n",
    "- matplotlib\n",
    "- photutils\n",
    "- astroscrappy\n",
    "- [STDPipe](https://github.com/karpov-sv/stdpipe/) (and its dependencies)\n",
    "\n",
    "*STDPipe* may be installed following the instructions at https://stdpipe.readthedocs.io/en/latest/installation.html, or just by doing \n",
    "```\n",
    "pip install stdpipe\n",
    "``` \n",
    "\n",
    "## External dependencies\n",
    "\n",
    "The tutorial code will require several external packages to be installed:\n",
    "\n",
    " - [SExtractor](https://github.com/astromatic/sextractor)\n",
    " - [SCAMP](https://github.com/astromatic/scamp)\n",
    " - [SWarp](https://github.com/astromatic/swarp)\n",
    " - [PSFEx](https://github.com/astromatic/psfex)\n",
    " - [HOTPANTS](https://github.com/acbecker/hotpants)\n",
    "\n",
    "Most of them may be installed directly from package managers, e.g. in Ubuntu or Debian as\n",
    "```\n",
    "sudo apt install sextractor scamp psfex swarp\n",
    "```\n",
    "or, in Miniconda/Anaconda environment, like that:\n",
    "```\n",
    "conda install -c conda-forge astromatic-source-extractor astromatic-scamp astromatic-psfex astromatic-swarp\n",
    "```\n",
    "\n",
    "*HOTPANTS* cannot be installed this way, and has to be compiled manually. There is an automated script for doing it available at https://github.com/karpov-sv/stdpipe/blob/master/install_hotpants.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fe424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Astropy version is less than 5.3 - as it breaks the support for compressed FITS files from Pan-STARRS\n",
    "# %pip install astropy==5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaed422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import matplotlib.pyplot as plt\n",
    "# Some defaults for Matplotlib\n",
    "plt.rc('image', origin='lower', cmap='Blues_r')\n",
    "plt.rc('figure', figsize=[12, 8])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# AstroPy ecosystem\n",
    "from astropy.wcs import WCS\n",
    "from astropy.io import fits as fits\n",
    "\n",
    "# Photutils\n",
    "import photutils\n",
    "\n",
    "# Cosmic ray removal using LACosmic algorithm\n",
    "import astroscrappy\n",
    "\n",
    "# Disable some annoying warnings from astropy (optional)\n",
    "import warnings\n",
    "from astropy.wcs import FITSFixedWarning\n",
    "warnings.simplefilter(action='ignore', category=FITSFixedWarning)\n",
    "from astropy.utils.exceptions import AstropyUserWarning\n",
    "warnings.simplefilter(action='ignore', category=AstropyUserWarning)\n",
    "# Also silence FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdc834",
   "metadata": {},
   "source": [
    "*STDPipe* (Simple Transient Detection Pipeline) is a set of routines intended to help with common tasks during astronomical image analysis. It also conveniently wraps external packages that do not have their own Python interfaces, such as SExtractor, SWarp, SCAMP or HOTPANTS. We will use it to simplify the code, but the same may be of course done directly by calling corresponding executables and loading their results.\n",
    "\n",
    "We will primarily use the following routines from *STDPipe*:\n",
    "- `plots.imshow` - drop-in replacement for Matplotlib `imshow` with better intensity scaling\n",
    "- `photmetry.get_objects_sextractor` - wrapper for running SExtractor and getting its result as an Astropy Table object\n",
    "- `photometry.measure_objects` - forced aperture photometry with optional sky annulus background subtraction\n",
    "- `catalogs.get_cat_vizier` - for getting the catalogues from Vizier database\n",
    "- `pipeline.refine_astrometry` - for refining existing astrometric solution using SCAMP\n",
    "- `pipeline.calibrate_photometry` - for deriving the photometric solution\n",
    "- `plots.plot_photometric_match` - for plotting the results of photometric calibration\n",
    "- `templates.get_survey_image` - for downloading template images from Pan-STARRS, and aligning them with science image\n",
    "- `subtraction.run_hotpants` - for subtracting the images using HOTPANTS\n",
    "- `pipeline.filter_transient_candidates` - for filtering the transient candidates based on simple criteria\n",
    "- `cutouts.get_cutout` - for making the cutouts from the image around transient positions\n",
    "- `plots.plot_cutout` - for displaying the cutouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b4d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure STDPipe is installed\n",
    "# %pip install -U stdpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b20f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check external dependencies\n",
    "\n",
    "!which source-extractor scamp swarp hotpants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aae2ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load STDPipe sub-modules\n",
    "from stdpipe import (\n",
    "    astrometry,\n",
    "    photometry, \n",
    "    catalogs, \n",
    "    cutouts,\n",
    "    templates,\n",
    "    subtraction, \n",
    "    plots, \n",
    "    psf, \n",
    "    pipeline, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7d0e6",
   "metadata": {},
   "source": [
    "# Load and pre-process the science image\n",
    "\n",
    "The tutorial (and all reasonable data processing in general) expects as an input the *science-ready* image, cleaned as much as possible from instrumental signatures and imaging artefacts. In practice, it means that the image should be\n",
    " - bias and dark subtracted\n",
    " - flat-fielded.\n",
    "\n",
    "Also, the artefacts such as saturated stars, bleeding charges, cosmic ray hits etc have to be masked.\n",
    "\n",
    "Often all this information is distributed directly along with the image, usually as a separate 'mask' file.\n",
    "But here I will show how to perform basic masking of the image manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a447c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ptf_sn_2012p.fits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the image and parse its header!\n",
    "image = fits.getdata(filename).astype(np.double)\n",
    "header = fits.getheader(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4ee38",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Inspect the header and see what filter was used for the image. Also, see whether CCD gain is present in some keywords, and what is its value. Remember, the gain is critical for proper image analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf63f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some parameters from FITS header\n",
    "fname = header.get('FILTER', 'unknown')\n",
    "gain = header.get('GAIN', 1.0)\n",
    "\n",
    "print('Processing %s: filter %s gain %.2f' \n",
    "      % (filename, fname, gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80715167",
   "metadata": {},
   "source": [
    "*STDPipe* have some convenience plotting functions collected inside `plots` module. Here we will use one of them, `plots.imshow`, which is a thin wrapper around standard Matplotlib `imshow`, but with quantile-based scaling, optional logarithmic/asinh stretching, buili-in colorbar, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(image)\n",
    "plt.title('Pre-processed image');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd6c4a",
   "metadata": {},
   "source": [
    "Now we will estimate the saturation level as a 95% quantile above the median level, and mask everything above it. We will also mask cosmic rays in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48100828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple estimate of saturation level\n",
    "saturation = 0.05*np.nanmedian(image) + 0.95*np.nanmax(image) # median + 0.95(max - median)\n",
    "print('Saturation level is %.1f ADU' % saturation)\n",
    "\n",
    "mask = image > saturation\n",
    "\n",
    "# Masking of cosmic rays using Astro-SCRAPPY \n",
    "\n",
    "# If the image was background-subtracted or otherwise pre-processed, the code will not \n",
    "# be able to correctly guess its error model. So let's compute it manually\n",
    "bg = photutils.Background2D(image, 128, mask=mask)\n",
    "var = bg.background_rms**2 + np.abs(image - bg.background)/gain\n",
    "\n",
    "cmask, cimage = astroscrappy.detect_cosmics(\n",
    "    image, \n",
    "    mask, \n",
    "    satlevel=saturation,\n",
    "    invar=var.astype(np.float32), # Image variance\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "mask |= cmask\n",
    "\n",
    "print('%d (%.1f%%) pixels masked in total' % (np.sum(mask), 100.0*np.sum(mask)/image.shape[0]/image.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(mask)\n",
    "plt.title('Mask');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4b94f",
   "metadata": {},
   "source": [
    "# Detect and measure objects in the image\n",
    "\n",
    "We will use SExtractor for initial detection of the objects in the image. The wrapper function will automatically prepare necessary config files for it, run the executable, parse its results and cleanup all temporary files created. The wrapper allows passing any configuration option directly to SExtractor through `extra` argument, and get additional measured parameters listed in `extra_param` argument. It returns the results as a standard Astropy Table, ordered by the object brightness. See the full documentation for this wrapper function at https://stdpipe.readthedocs.io/en/latest/detection.html\n",
    "\n",
    "The detection in SExtractor is based on building the noise model through (grid-based) background and background rms estimation, and then extracting the groups of connected pixels above some pre-defined threshold. It does not use any prior information on the source shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = photometry.get_objects_sextractor(\n",
    "    image, \n",
    "    # Mask - the objects whose isophotal footprints intersect it will have a flag set\n",
    "    mask=mask,\n",
    "    # Initial aperture to be used for getting object flux\n",
    "    aper=3.0, \n",
    "    # Minimal area of the object to be detected, in pixels\n",
    "    minarea=5,\n",
    "    # Gain\n",
    "    gain=gain, \n",
    "    # Extra option to automatically reject all objects closer than 10 pixels to image edge.\n",
    "    edge=10,\n",
    "    # With this set, we will see how the executable is actually being called\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(len(obj), 'objects detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb8116",
   "metadata": {},
   "source": [
    "Let's check first three objects. The wrapper automatically converts some field names from original SExtractor ones to shorter names (e.g. `MAG_APER`->`mag`, `X_IMAGE`->`x`, `FLAGS`->`flags`, `FWHM`->`fwhm`, etc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31313f57",
   "metadata": {},
   "source": [
    "We may now overplot these detected objects on top of the image to check the quality of object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ad383",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(image, interpolation='bicubic')\n",
    "plt.plot(obj['x'], obj['y'], 'r.')\n",
    "plt.title('Detected objects');\n",
    "\n",
    "# We may optionally zoom into some region to better see the details\n",
    "# plt.xlim(1000, 1400)\n",
    "# plt.ylim(1000, 1400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b67c68",
   "metadata": {},
   "source": [
    "SExtractor does not allow to do forced photometry (i.e. measurement of an object at user-specified position), nor does it handle local background variations too well. So there is an additional (photutils-based) routine that we will use for forced photometry in a circular aperture with size defined by mean FWHM of the stars in the image. We will also be able to use the same routine with the same parameters for doing the photometry on difference image, in order to get consistent photometric results.\n",
    "\n",
    "In the measurement routine, the flux is estimated as a sum of counts inside the aperture, minus sum of background values inside the same pixels. Background may be either:\n",
    " - global, estimated over a grid with size controlled throgh `bg_size` parameter in a way similar to how *SExtractor* is doing it\n",
    " - local, estimated as a sigma-clipped mean value inside the annulus with inner and outer radii set through `bkgann` parameter, and centered on the object. \n",
    " \n",
    "The error budget of photometric measurements consists of:\n",
    " - background noise inside the aperture - this part is empirically estimated and is quite robust\n",
    " - Poissonian noise of the source itself. This component is sensitive to knowing exact gain of the detector used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3af97a",
   "metadata": {},
   "source": [
    "But first, we should get a rough estimation of the seeing. For that, we will first use median FWHM of detected objects, taking into account only unflagged (e.g. not saturated) ones. This method is not *exactly* accurate, as you will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d68aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwhm = np.median(obj['fwhm'][obj['flags'] == 0])\n",
    "print('Median FWHM is %.1f pixels' % fwhm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bdcea9",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Check the accuracy of a rough FWHM estimation we made from simple median, and correct it if necessary. Refer to the following figure for general principle (the figure is from [PSFEx documentation](https://psfex.readthedocs.io/en/latest/index.html)).\n",
    "\n",
    "PSFEx documentation suggests using the mode of the distribution for the objects above S/N > 20 and below the saturation limit. Mode may be (sometimes) approximated as a `3.0*median - 2.0*mean` for the sigma-clipped subset of a distribution. If you are brave enough, try to modify the code below to use this approach. Or just guess the correct value manually.\n",
    "\n",
    "![](https://psfex.readthedocs.io/en/latest/_images/rhmag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude saturated objects etc\n",
    "idx = obj['flags'] == 0\n",
    "\n",
    "# Also exclude lower-quality objects\n",
    "idx1 = idx & (obj['magerr'] < 1/20) \n",
    "fwhm = np.median(obj['fwhm'][idx1]) # It should be the mode instead of median!\n",
    "\n",
    "#fwhm = ????\n",
    "\n",
    "plt.plot(obj['fwhm'][idx], obj['mag'][idx], '.')\n",
    "plt.axvline(np.median(obj['fwhm'][idx]), ls=':', color='red', label='Initial FWHM value')\n",
    "plt.axvline(fwhm, ls='--', color='red', label='Correct FWHM value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('FWHM, pixels')\n",
    "plt.ylabel('Instrumental magnitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5605f81",
   "metadata": {},
   "source": [
    "We will pass this FWHM to measurement function so that aperture and background radii will be relative to it (if it is not passed, the radii are expected to be in pixels).\n",
    "\n",
    "The radius of photometric aperture should be selected so that it\n",
    " - maximizes the fraction of object flux that falls inside it\n",
    " - minimizes the background noise contribution\n",
    " - minimizes the chance to also capture the flux from nearby objects\n",
    " \n",
    "Typically, aperture radius of 1.5 to 2 FWHM is considered optimal. However, we will take just 1.0 FWHM for it.\n",
    "\n",
    "We will also reject all objects with measured S/N < 5, as they are mostly useless for any calibration (and also cannot even be considered reliably detected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75794b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = photometry.measure_objects(\n",
    "    obj, \n",
    "    image, \n",
    "    mask=mask, \n",
    "    fwhm=fwhm, # It will be used for scaling the aperture radius\n",
    "    gain=gain, # It will be used for computing Poissonian noise contribution\n",
    "    aper=1.0, # In FWHM units\n",
    "    bkgann=[5, 7], # In FWHM units\n",
    "    sn=5, # Minimal S/N to be accepted\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(len(obj), 'objects properly measured')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0297f2c9",
   "metadata": {},
   "source": [
    "# Astrometric calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d468a8",
   "metadata": {},
   "source": [
    "For simplicity, here we will assume that the image is already (roughly) astrometrically calibrated, i.e. that its FITS header contains some WCS solution allowing to map pixel coordinates to sky positions. Such astrometric calibration may be done e.g. using [Astrometry.Net](https://nova.astrometry.net) website. *STDPipe* also contains wrapper functions to perform such blind matching (`astrometry.blind_match_objects` and `astrometry.blind_match_astrometrynet`, see https://stdpipe.readthedocs.io/en/latest/astrometry.html), but we will not use them here.\n",
    "\n",
    "Instead, we will just load the solution from FITS header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial WCS\n",
    "wcs = WCS(header)\n",
    "\n",
    "# Get the center position, size and pixel scale for the image\n",
    "center_ra,center_dec,center_sr = astrometry.get_frame_center(\n",
    "    wcs=wcs, \n",
    "    width=image.shape[1], \n",
    "    height=image.shape[0]\n",
    ")\n",
    "\n",
    "pixscale = astrometry.get_pixscale(wcs=wcs)\n",
    "\n",
    "print('Frame center is %.2f %.2f radius %.2f deg, %.2f arcsec/pixel' \n",
    "      % (center_ra, center_dec, center_sr, pixscale*3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f242d0",
   "metadata": {},
   "source": [
    "# Reference catalogue\n",
    "\n",
    "Now we may download the catalogue for this sky region from Vizier using a function `catalogs.get_cat_vizier` from *STDPipe*. The routine uses [astroquery](https://astroquery.readthedocs.io/en/latest/) package to send actual queries to data archives.\n",
    "\n",
    "Catalogue name may be any Vizier identifier, or one of supported shortcuts for popular choices (**ps1, gaiadr2, gaiaedr3, gaiadr3syn, usnob1, gsc, skymapper, apass, sdss, atlas, vsx** etc). The full list of supported catalogue aliases is in `catalogs.catalogs` dictionary.\n",
    "\n",
    "Column filters may be also provided to limit the output e.g. to brightest objects only. The format is the same as used on Vizier web site.\n",
    "\n",
    "Some of catalogue shortcuts (e.g. **ps1** and **gaiadr2**) also initiate augmenting the returned catalogue with photometric data not originally stored there, most notably - Johnson-Cousins **B, V, R and I** magnitudes. The transformations used for that are either taken from literature, or derived by cross-matching these catalogues with a large set of Landolt standard stars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243e110",
   "metadata": {},
   "source": [
    "The depth of the catalogue we will request should roughly correspond to the magnitude of the faint stars in our image - if it is much deeper, we will get more spurious matches, and it may worsen the astrometric solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097b71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's get PanSTARRS objects brighter than r=22 mag\n",
    "cat = catalogs.get_cat_vizier(\n",
    "    # Center position\n",
    "    center_ra, \n",
    "    center_dec, \n",
    "    # Radius in degree\n",
    "    center_sr, \n",
    "    # Catalogue name or alias\n",
    "    'ps1', \n",
    "    # Column filters \n",
    "    filters={'rmag':'<22'}\n",
    ")\n",
    "\n",
    "print(len(cat), 'catalogue stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abce56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93f8b2",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Overplot the catalogue over the image and check whether the stars match with catalogue entries, and whether the depth is comparable. If not, you may adjust the limit we set in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fe23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(image, interpolation='bicubic')\n",
    "plt.title('Catalogue stars')\n",
    "\n",
    "plt.autoscale(False)\n",
    "\n",
    "x,y = wcs.all_world2pix(cat['RAJ2000'], cat['DEJ2000'], 0)\n",
    "plt.plot(x, y, 'r.')\n",
    "\n",
    "# We may optionally zoom into some region to better see the details\n",
    "# plt.xlim(0, 100)\n",
    "# plt.ylim(900, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c7a01",
   "metadata": {},
   "source": [
    "# Astrometric refinement\n",
    "\n",
    "Image subtraction is extremely sensitive to the alignment of images, and it needs the astrometric solution as accurate as possible. Therefore, we will try to refine the astrometric solution based on the positions of detected objects and catalogue stars using *SCAMP* code - again, through *STDPipe* wrapper for it (that also forces it to use our already downloaded catalogue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SCAMP for astrometric refinement.\n",
    "wcs = pipeline.refine_astrometry(\n",
    "    # List of detected objects\n",
    "    obj, \n",
    "    # List of catalogue stars\n",
    "    cat, \n",
    "    # Matching radius\n",
    "    10*pixscale,\n",
    "    # Initial astrometric solution\n",
    "    wcs=wcs, \n",
    "    # Polynomial order for distortions\n",
    "    order=5,\n",
    "    # Method to use - we need to select SCAMP here\n",
    "    method='scamp', \n",
    "    # Catalogue column name with the closest magnitude\n",
    "    cat_col_mag='imag', \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if wcs is not None:\n",
    "    # Remove astrometric keywords from existing header\n",
    "    astrometry.clear_wcs(\n",
    "        header, \n",
    "        remove_comments=True, \n",
    "        remove_underscored=True, \n",
    "        remove_history=True\n",
    "    )\n",
    "    \n",
    "    # Update WCS info in the header\n",
    "    header.update(wcs.to_header(relax=True))\n",
    "else:\n",
    "    print(\"Astrometric refinement failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07165bdb",
   "metadata": {},
   "source": [
    "# Photometric calibration\n",
    "\n",
    "Photometric calibration is performed by positionally matching detected objects with catalogue stars, and then building the photometric model for their instrumental magnitudes.\n",
    "\n",
    "The model includes:\n",
    " - catalogue magnitude\n",
    " - (optionally spatially varying) zero point\n",
    " - (optionally) catalogue color\n",
    " - plus, of course, measurement errors\n",
    " \n",
    "and looks like that:\n",
    "$$ \n",
    "\\mbox{Catalogue} = \\mbox{Instrumental} + C\\cdot\\mbox{color} + \\mbox{polynomial}(x, y, degree) + \\mbox{err}\n",
    "$$\n",
    "\n",
    "where $\\mbox{Catalogue}$ is e.g. $\\mbox{V}$ magnitude and $\\mbox{color}$ is e.g $\\mbox{B-V}$\n",
    "\n",
    "This is equivalent to the instrumenmtal photometric system defined as $\\mbox{V} - C\\cdot(\\mbox{B-V})$. Thus, the color term captures the difference between the catalogue and instrumental photometric systems, e.g. due to different filters used, etc.\n",
    "\n",
    "While the calibration may be done manually in simple cases (e.g. by just taking median difference between instrumental and catalogue magnitudes as a zero point), it would be easier to just use *STDPipe* high-level routine `pipeline.calibrate_photometry` for handling it. The routines perform an iterative fitting with rejection of pairs deviating too much (more than `threshold` sigmas) from the model. Optional intrinsic scatter (specified through `max_intrinsic_rms` parameter) may also help accounting for the effects of e.g. multiplicative noise (flatfielding, subpixel sensitivity variations, etc). Its full documentation is at https://stdpipe.readthedocs.io/en/latest/photometry.html\n",
    "\n",
    "The routine returns the structure with various fields representing e.g. the indices of the objects actually used for the calibration, measured and fitted zero points for individual objects, etc. They all may be used to make various diagnostic plots showing the quality of the fit, and we will see it in more details later.\n",
    "But the main output is in the field `zero_fn` - it contains the callable that returns the fitted zero point at an arbitrary position within the image. It may be used to convert e.g. measured transient flux (instrumental magnitdue) to calibrated magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c24657",
   "metadata": {},
   "source": [
    "For photometric calibration, we have to select the correct 'primary' catalogue magnitude, with the filter equal (or closest) to the one used for our actual image. The color term, on the other hand, may be in most cases left to (for Pan-STARRS reference catalogue) `gmag-rmag`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photometric calibration using 2 arcsec matching radius, \n",
    "# r magnitude, g-r color and second order spatial variations\n",
    "m = pipeline.calibrate_photometry(\n",
    "    obj, \n",
    "    cat, \n",
    "    # Matching radius, in degrees\n",
    "    sr=2/3600, \n",
    "    # Primary filter column name\n",
    "    cat_col_mag='imag',\n",
    "    # Color term\n",
    "    cat_col_mag1='gmag', \n",
    "    cat_col_mag2='rmag', \n",
    "    # If set to True, it will include color term in the model\n",
    "    use_color=False,\n",
    "    # Allow some scatter on top of formal error bars\n",
    "    max_intrinsic_rms=0.02, \n",
    "    # Spatial order of the model - select 0 if we have few stars, otherwise 2 is enough\n",
    "    order=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# The code above automatically augments the object list \n",
    "# with calibrated magnitudes, but we may also do it manually\n",
    "obj['mag_calib'] = obj['mag'] + m['zero_fn'](\n",
    "    obj['x'], \n",
    "    obj['y']\n",
    ")\n",
    "# Zeropoint model errorr is typically small, but we may still add it\n",
    "obj['mag_calib_err'] = np.hypot(\n",
    "    obj['magerr'], m['zero_fn'](\n",
    "        obj['x'], \n",
    "        obj['y'], \n",
    "        get_err=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b93fd5",
   "metadata": {},
   "source": [
    "## Plotting calibration results\n",
    "\n",
    "There is a handy plotting function useful for quick checking of fitting quality and uncorrected trends. It marks the stars used for final model fit with red dots, and flagged (e.g. saturated) stars rejected from the fitting from the start - with yellow diagonal crosses. The rest of points are the ones that got rejected during the iterative model fitting process.\n",
    "\n",
    "Upper panel shows the model fitting residuals as a function of magnitude. It should be centered on zero, within error bars. \n",
    "\n",
    "Lower panel is the same but w.r.t. the color. If the fiilter is selected properly, or if color term is included in the fitting, it also will be around zero. If color term is included, the legend will show the actual photmetric system of the image. Color term should be small - if it is larger than 0.5, it usually mean that the primary filter is selected incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71223fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Photometric residuals as a function of catalogue magnitude\n",
    "plt.subplot(211)\n",
    "plots.plot_photometric_match(m)\n",
    "plt.ylim(-0.5, 0.5)\n",
    "\n",
    "# Photometric residuals as a function of catalogue color\n",
    "plt.subplot(212)\n",
    "plots.plot_photometric_match(m, mode='color')\n",
    "plt.ylim(-0.5, 0.5)\n",
    "plt.xlim(0.0, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f14d1",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Does the calibration look good? Is there any uncorrected color (or magnitude) trend? If so, try to use different primary color for the calibration. Also try to turn on and off the color term fitting to see what happens to the trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99b5fd",
   "metadata": {},
   "source": [
    "### Optional - check the spatial dependence of the zero point\n",
    "\n",
    "The zero point may have spatial variations in several cases - e.g. when your flat field was not correct, or when your PSF (star shape) varies over the field of view, and your aperture size is small. In the latter case, the fraction of the star flux within the aperture (also called `aperture correction`) will vary with the position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero point (difference between catalogue and instrumental magnitudes for every star) map\n",
    "plt.subplot(121)\n",
    "plots.plot_photometric_match(\n",
    "    m, \n",
    "    mode='zero', \n",
    "    bins=6, \n",
    "    # Whether to show positions of the stars\n",
    "    show_dots=True, \n",
    "    color='red', \n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "plt.title('Zero point')\n",
    "\n",
    "# Fitted zero point model with second-order spatial polynomial term\n",
    "plt.subplot(122)\n",
    "plots.plot_photometric_match(\n",
    "    m, \n",
    "    mode='model', \n",
    "    bins=6, \n",
    "    show_dots=True, \n",
    "    color='red', \n",
    "    aspect='equal'\n",
    ")\n",
    "\n",
    "plt.title('Zero point model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1734f82",
   "metadata": {},
   "source": [
    "## Simple catalogue-based transient detection\n",
    "\n",
    "Anyway, at this point we already have the detections of all sufficiently isolated and sufficiently bright objects on the frame, as well as an astrometric and photometric calibration for them. \n",
    "\n",
    "Some transients may already be detected by comparing the detected objects with catalogue, and selecting the ones that do not have any counterpart there. This approach is limited, as\n",
    " - it does not play well with blended objects, as they are often being detected as a single extended thing with centroid separated from the centres of individual components\n",
    " - it does not easily allow to detect flares or low-amplitude variability of objects that are present in the catalogue\n",
    " \n",
    "Anyway, let's do it. We have a dedicated routine `pipeline.filter_transient_candidates` that accepts the list of objects, and filters out the following things:\n",
    " - flagged ones, i.e. with `obj['flags'] != 0`\n",
    " - positionally coincident with stars from provided cataloge table (if `cat != None`)\n",
    " - positionally coincident with stars from Vizier catalogues specified as a list of names (if `vizier` list is non-empty)\n",
    " - positionally and temporally coincident with Solar system objects from SkyBoT service (if `skybot = True` and `time` is provided, or is present in the object list as a column)\n",
    " - positionally coincident with NED objects (if `ned = True`)\n",
    "\n",
    "As we are already using PanSTARRS as a reference catalogue, let's limit Vizier filtering to variable stars only. And as an input list of candidates, let's just use all detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering of transient candidates\n",
    "candidates = pipeline.filter_transient_candidates(\n",
    "    obj,\n",
    "    cat=cat, \n",
    "    sr=0.5*fwhm*pixscale,\n",
    "    # We will check only against Pan-STARRS as our reference catalogue may not be deep enough\n",
    "    vizier=['ps1'],\n",
    "    # Filter out any flags except for 0x100 which is isophotal masked\n",
    "    flagged=True, \n",
    "    flagmask=0xfe00,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1024218",
   "metadata": {},
   "source": [
    "Now we may create cutouts for these surviving candidates and vizualize them, also downloading corresponding cutouts from PanSTARRS itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b78a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in candidates:\n",
    "    # Create the cutout from image based on the candidate\n",
    "    cutout = cutouts.get_cutout(\n",
    "        # Image\n",
    "        image, \n",
    "        # Candidate - it will use its pixel position to center the cutout\n",
    "        cand, \n",
    "        # Cutout half-size in pixels\n",
    "        20,\n",
    "        # Additional planes for the cutout\n",
    "        mask=mask, \n",
    "        # FITS header to include with the cutout - it will automatically adjust WCS there\n",
    "        header=header\n",
    "    )\n",
    "    \n",
    "    # We did not do image subtraction yet, but we may already\n",
    "    # directly download the \"template\" image for this position \n",
    "    # from HiPS server - same scale and orientation, but different PSF shape!..\n",
    "    cutout['template'] = templates.get_hips_image(\n",
    "        # HiPS survey name - anything from http://aladin.cds.unistra.fr/hips/list goes!\n",
    "        'PanSTARRS/DR1/r',\n",
    "        # FITS header with proper WCS and size\n",
    "        header=cutout['header'],\n",
    "        # Return only image\n",
    "        get_header=False \n",
    "    )\n",
    "    \n",
    "    # Pixel position of the candidate inside the cutout\n",
    "    x0,y0 = WCS(cutout['header']).all_world2pix(cand['ra'], cand['dec'], 0)\n",
    "    \n",
    "    # Now we have three image planes in the cutout - let's display them\n",
    "    plots.plot_cutout(\n",
    "        cutout, \n",
    "        # Image planes to display - optional, by default displays everything\n",
    "        planes=['image', 'template', 'mask'],\n",
    "        # Percentile-based scaling and linear stretching\n",
    "        qq=[0.5, 99.95], \n",
    "        # Intensity stretching, may be 'linear', 'asinh', 'log', 'power' etc\n",
    "        stretch='power',\n",
    "        # Mark the candidate position\n",
    "        mark_x=x0,\n",
    "        mark_y=y0,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e41524",
   "metadata": {},
   "source": [
    "# Image subtraction\n",
    "\n",
    "More accurate and stable transient detection is performed using *image subtraction* which is supposed to remove the contribution from all non-transient and non-variable objects in the image, including stars, galaxies, etc.\n",
    "\n",
    "In addition to what we already have (science image, its mask, its astrometric and photometric calibration) we will need the following to perform the image subtraction:\n",
    " - template image, astrometrically aligned with our science image\n",
    " - mask for the template\n",
    " - noise models for both the template and science images\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d2590d",
   "metadata": {},
   "source": [
    "## Templates from public data archives\n",
    "\n",
    "The template may be some image or coadd from the same instrument that does not contain the transient - e.g. acquired after it already faded, or well before its onset. The latter is how modern time-domain sky surveys handle it - they first collect the templates for all sky regions they will operate upon, and then routinely monitor these parts of the sky. But for smaller instruments you typically do not have prior observations of your sky field. Fortunately, nowadays you may just download sufficiently deep images for nearly any point of the sky from public data archives (but it will be, of course, from different instrument and probably in slightly different filter). We will show how to use this approach to prepare the template for subtraction.\n",
    "\n",
    "The images may be acquired manually, and then astrometrically aligned with science image using either [SWarp](https://github.com/astromatic/swarp), or Python libraries like [AstroAlign](https://github.com/quatrope/astroalign) or [MontagePy](http://montage.ipac.caltech.edu/docs/montagePy-UG.html).\n",
    "\n",
    "*STDPipe* contains the convenience routine for downloading the images for both deep coadds and corresponding masks from either [Pan-STARRS](https://outerspace.stsci.edu/display/PANSTARRS/Pan-STARRS1+data+archive+home+page) or [DESI Legacy Survey](https://www.legacysurvey.org), and its automatic re-projection (using SWarp) onto the pixel grid of science image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370dc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get r band image from Pan-STARRS with the same resolution and orientation\n",
    "tmpl = templates.get_survey_image(\n",
    "    # Photometric band - one of 'g', 'r', 'i', 'z', 'y' for Pan-STARRS\n",
    "    band='i',\n",
    "    # One of 'image' or 'mask'\n",
    "    ext='image',\n",
    "    # Either 'ps1' for Pan-STARRS or 'ls' for Legacy Survey\n",
    "    survey='ps1',\n",
    "    # pixel grid defined by WCS and image size\n",
    "    wcs=wcs,\n",
    "    width=image.shape[1],\n",
    "    height=image.shape[0],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Also get proper mask\n",
    "tmask = templates.get_survey_image(\n",
    "    # Photometric band - one of 'g', 'r', 'i', 'z', 'y' for Pan-STARRS\n",
    "    band='i',\n",
    "    # One of 'image' or 'mask'\n",
    "    ext='mask',\n",
    "    # Either 'ps1' for Pan-STARRS or 'ls' for Legacy Survey\n",
    "    survey='ps1',\n",
    "    # pixel grid defined by WCS and image size\n",
    "    wcs=wcs,\n",
    "    width=image.shape[1],\n",
    "    height=image.shape[0],\n",
    "    verbose=True\n",
    ")\n",
    "# Mask bits for Pan-STARRS are documented at https://outerspace.stsci.edu/display/PANSTARRS/PS1+Pixel+flags+in+Image+Table+Data\n",
    "# We will exclude pixels with any non-zero mask value\n",
    "tmask = tmask > 0\n",
    "# We will also mask the regions of the template filled with NaNs\n",
    "tmask |= np.isnan(tmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plots.imshow(tmpl, show_colorbar=False)\n",
    "plt.title('Template')\n",
    "\n",
    "plt.subplot(122)\n",
    "plots.imshow(tmask, show_colorbar=False)\n",
    "plt.title('Template mask')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5708b06",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the template alongside with original image, and see how much deeper and sharper is it. Or is it not?.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cb73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(121)\n",
    "plots.imshow(image, interpolation='bicubic')\n",
    "plt.title('Original image')\n",
    "\n",
    "plt.subplot(122, sharex=ax, sharey=ax) # So that zoom works for both images\n",
    "plots.imshow(tmpl, interpolation='bicubic')\n",
    "plt.title('Template')\n",
    "\n",
    "# plt.xlim(800, 1200)\n",
    "# plt.ylim(800, 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495acd26",
   "metadata": {},
   "source": [
    "# Subtracting the images with HOTPANTS\n",
    "\n",
    "We will show how to use [HOTPANTS](https://github.com/acbecker/hotpants) (which stands for \"High Order Transform of PSF ANd Template Subtraction\") code for image subtraction. It implements a variant of [Alard and Lupton (1998)](https://ui.adsabs.harvard.edu/abs/1998ApJ...503..325A/abstract) algorithm that convolves one of the images with the kernel composed of several simple basis functions, so that it matches the resolution (and flux scale) of the second image. \n",
    "\n",
    "Image subtraction with HOTPANTS is implemented in `subtraction.run_hotpants` routine, that implements the following logic on top of just running `hotpants` binary:\n",
    " - finding optimal convolutuon kernel sizes based on image and template FWHM (three Gaussians, with sizes 0.5, 1 and 2 `sigma_match`, where `sigma_match = sqrt(image_fwhm ** 2 - template_fwhm ** 2) / 2.35`, and with the orders of corresponding polynomials 6, 4 and 2).\n",
    " - filling in necessary parameters like min/max pixel values, etc\n",
    "     - also, all parameters may be overwritten or added by passing them through `extra` argument, allowing full control over the code\n",
    " - (optionally) building noise models for the image and template based on their background statistics and gain, so that they may be e.g. background-subtracted\n",
    "     - noise model is again based on background rms + Poisson contribution from the object flux\n",
    " - (optionally) constructing the list of stamp positions used for fitting the convolution kernel based on user-supplied list of coordinates (e.g. detected and unflagged objects)\n",
    "\n",
    "By default, it will convolve the template, so that the difference image will have the same resolution and flux scaling as the original science image. This way, we may directly use our photmetric zero point for the aperture photometry results derived on difference image. For it to work properly, the template has to be sharper (with smaller FWHM) than the image itself.\n",
    "\n",
    "HOTPANTS computes, and the routine optionally returns - along with difference image - the template convolved with the kernel (so supposedly matching the science image resolution), noise-scaled difference image, and the error model. The latters may be used to assess the significance of detections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bc1e7",
   "metadata": {},
   "source": [
    "***\n",
    "In order to ease the subtraction, we will need to remove any additional inhomogeneities in the image - caused e.g. by non-uniform sky background due to Moon or some reflections. For that, we will subtract the smoothly varying backgrounds, computed with the same grid size, from both the image and template, so that the impact on actual large-scale structures (nearby galaxies?..) was similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636bb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for the subtraction by estimating image and template backgrounds\n",
    "bg = photutils.Background2D(\n",
    "    image,\n",
    "    # Grid size in pixels\n",
    "    128,\n",
    "    mask=mask\n",
    ").background\n",
    "\n",
    "template_bg = photutils.Background2D(\n",
    "    tmpl, \n",
    "    # Grid size in pixels\n",
    "    128,\n",
    "    mask=tmask\n",
    ").background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5383dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to estimate FWHM in the template\n",
    "tobj = photometry.get_objects_sextractor(\n",
    "    tmpl, \n",
    "    mask=tmask, \n",
    "    gain=1e6,\n",
    ")\n",
    "\n",
    "template_fwhm = np.median(tobj['fwhm'][tobj['flags'] == 0])\n",
    "print('Template FWHM is %.1f pixels' % template_fwhm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8415e",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Again, check the accuracy of FWHM estimation. It is important, as the sizes of convolution kernel basis functions depend on it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude saturated objects etc\n",
    "idx = tobj['flags'] == 0\n",
    "\n",
    "# Also exclude lower-quality objects\n",
    "idx1 = idx & (tobj['magerr'] < 1/20) \n",
    "template_fwhm = np.median(tobj['fwhm'][idx1])\n",
    "\n",
    "#template_fwhm = ????\n",
    "\n",
    "plt.plot(tobj['fwhm'][idx], tobj['mag'][idx], '.')\n",
    "plt.axvline(np.median(tobj['fwhm'][idx]), ls=':', color='red', label='Initial FWHM value')\n",
    "plt.axvline(template_fwhm, ls='--', color='red', label='Correct FWHM value')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('FWHM, pixels')\n",
    "plt.ylabel('Instrumental magnitude')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b2328",
   "metadata": {},
   "source": [
    "Now, the main function! It has a lot of parameters - see https://stdpipe.readthedocs.io/en/latest/subtraction.html#running-image-subtraction for the full list. And remember - image subtraction is an art, not proper science! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c4863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the subtraction and get all possible result planes\n",
    "diff,conv,sdiff,ediff = subtraction.run_hotpants(\n",
    "    # Background-subtracted image\n",
    "    image - bg, \n",
    "    # Background-subtracted template\n",
    "    tmpl - template_bg, \n",
    "    \n",
    "    # Masks\n",
    "    mask=mask, \n",
    "    template_mask=tmask,\n",
    "    \n",
    "    # FWHMs for the convolution kernel size\n",
    "    image_fwhm=fwhm, \n",
    "    template_fwhm=template_fwhm, \n",
    "    \n",
    "    # Parameters for the noise model\n",
    "    image_gain=gain, \n",
    "    template_gain=1e6, # Assume negligibly small Poissonian noise\n",
    "    \n",
    "    # Estimate noise model for the image automatically, using background and Poissonian noise\n",
    "    err=True, \n",
    "    template_err=True,\n",
    "    \n",
    "    # Output parameters\n",
    "    get_convolved=True, # Template convolved with the kernel\n",
    "    get_scaled=True, # Noise-scaled difference image\n",
    "    get_noise=True, # Error model for the difference image\n",
    "    \n",
    "    # Extra parameters to be passed to HOTPANTS (optional)\n",
    "    # e.g to split it into sub-images for independent processing:\n",
    "    #extra={'okn':True, 'nrx':2, 'nry':2},\n",
    "    # or, to manually specify the basis functions (see documentation)\n",
    "    #extra={'ng':[4, 6, 0.5, 4, 1.0, 2, 2.0, 2, 4.0]},\n",
    "    \n",
    "    # We may also provide the list of positions for deriving the kernel - otherwise \n",
    "    # it will try to place them automatically, which may be suboptimal sometimes\n",
    "    #obj=obj[obj['flags']==0],\n",
    "\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Bad pixels as marked by HOTPANTS\n",
    "dmask = diff == 1e-30 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b2c38d",
   "metadata": {},
   "source": [
    "Let's see the results. We will now plot the noise-scaled difference image, where every pixel value is divided by its estimated error - so, it represents the significance of the residual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to zero all masked pixels\n",
    "sdiff[mask | tmask | dmask] = 0\n",
    "\n",
    "plots.imshow(sdiff, vmin=-3, vmax=10)\n",
    "plt.title('Noise-scaled difference image')\n",
    "\n",
    "# plt.xlim(800, 1200)\n",
    "# plt.ylim(800, 1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a109e",
   "metadata": {},
   "source": [
    "Not every star got fully subtracted - these are mostly saturated objects that are masked. They will be rejected on the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120701a",
   "metadata": {},
   "source": [
    "# Transient detection in difference image\n",
    "\n",
    "Transient detection on difference image differs from the one we did on original image in two ways:\n",
    " - there are much less remaining sources\n",
    " - most of them are subtraction artefacts that have different PSF shape, so may be filtered out\n",
    " - we know the noise model for the difference image\n",
    " \n",
    "Knowing noise model also helps filtering out subtraction artefacts as they mostly happen in high-noise regions (inside stellar footprints).\n",
    "\n",
    "So we will pass this noise model to SExtractor to be used for transient detection. We will also show how to pass extra parameters to SExtractor to get extra fields in the output, inspect checkimages, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f5785",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the difference image alongside with original image, and see whether the stars are actually being subtracted. What kinds of problems are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f15973",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(121)\n",
    "plots.imshow(image, interpolation='bicubic')\n",
    "plt.title('Original image')\n",
    "\n",
    "plt.subplot(122, sharex=ax, sharey=ax) # So that zoom works for both images\n",
    "plots.imshow(diff, interpolation='bicubic')\n",
    "plt.title('Difference')\n",
    "\n",
    "# plt.xlim(800, 1200)\n",
    "# plt.ylim(800, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb74796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SExtractor on difference image with custom noise model, \n",
    "# returning object footprints and some additional fields\n",
    "sobj,segm = photometry.get_objects_sextractor(\n",
    "    # Operate on difference image\n",
    "    diff, \n",
    "    # Combined mask\n",
    "    mask=mask | tmask | dmask, \n",
    "    # Error model\n",
    "    err=ediff,\n",
    "    # Exclude everything closer than 20 pixels to the edge\n",
    "    edge=20,\n",
    "    wcs=wcs, \n",
    "    aper=5.0,\n",
    "    # Get extra object parameters\n",
    "    extra_params=[\n",
    "        'NUMBER', # Object number, the same as used in segmentation map\n",
    "    ], \n",
    "    # Pass extra configuration parameters to SExtractor\n",
    "    extra={}, \n",
    "    # Also return the segmentation map\n",
    "    checkimages=[\n",
    "        'SEGMENTATION'\n",
    "    ], \n",
    "    verbose=True)\n",
    "\n",
    "# Perform forced aperture photometry, again with custom noise model and forced zero background level\n",
    "sobj = photometry.measure_objects(\n",
    "    sobj, \n",
    "    # Operate on difference image\n",
    "    diff, \n",
    "    # Combined mask\n",
    "    mask=mask | tmask | dmask, \n",
    "    # Known noise model for difference image\n",
    "    err=ediff,\n",
    "    # Here we use the same parameters as for forced photometry on original image\n",
    "    fwhm=fwhm, # It will be used for scaling the aperture radius\n",
    "    gain=gain, # It will be used for computing Poissonian noise contribution\n",
    "    aper=1.0, # In FWHM units\n",
    "    bkgann=[5, 7], # In FWHM units\n",
    "    # Filter out everything with S/N < 5\n",
    "    sn=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# The difference is in original image normalization, so we know photometric zero point\n",
    "sobj['mag_calib'] = sobj['mag'] + m['zero_fn'](\n",
    "    sobj['x'], \n",
    "    sobj['y']\n",
    ")\n",
    "sobj['mag_calib_err'] = np.hypot(\n",
    "    sobj['magerr'], m['zero_fn'](\n",
    "        sobj['x'], \n",
    "        sobj['y'], \n",
    "        get_err=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(len(sobj), 'transient candidates found in difference image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ba9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering of transient candidates\n",
    "candidates = pipeline.filter_transient_candidates(\n",
    "    sobj,\n",
    "    # Reject entries present in the reference catalogue\n",
    "    #cat=cat, \n",
    "    sr=0.5*fwhm*pixscale,\n",
    "    # We will not filter any catalogues, maybe except for AAVSO VSX\n",
    "    #vizier=['vsx'], \n",
    "    # Filter out any flags except for 0x100 which is isophotal masked\n",
    "    flagged=True, flagmask=0xfe00,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f833afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,cand in enumerate(candidates):\n",
    "    print('Candidate %d with mag = %.2f +/- %.2f at x/y = %.1f %.1d and RA/Dec = %.4f %.4f' \n",
    "          % (i, cand['mag_calib'], cand['mag_calib_err'], cand['x'], cand['y'], cand['ra'], cand['dec']))\n",
    "\n",
    "    cutout = cutouts.get_cutout(\n",
    "        image, \n",
    "        cand, \n",
    "        20, # You may vary the cutout size to better see the regions around the source\n",
    "        mask=mask | tmask | dmask, \n",
    "        diff=diff, \n",
    "        template=tmpl, \n",
    "        convolved=conv, \n",
    "        err=ediff, \n",
    "        footprint=(segm == cand['NUMBER']), \n",
    "        header=header, \n",
    "        filename=filename, \n",
    "    )\n",
    "\n",
    "    # Pixel position of the candidate inside the cutout\n",
    "    x0,y0 = WCS(cutout['header']).all_world2pix(cand['ra'], cand['dec'], 0)\n",
    "\n",
    "\n",
    "    plots.plot_cutout(\n",
    "        cutout, \n",
    "        planes=['image', 'template', 'convolved', 'diff', 'footprint', 'mask'], \n",
    "        qq=[0.5, 99.5], \n",
    "        stretch='linear',\n",
    "        # Mark the candidate position\n",
    "        mark_x=x0,\n",
    "        mark_y=y0,\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0200f7a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try to disable catalogue-based candidate filtering and see how they actually look like. Also, try to play with subtraction parameters. Try to download the \"wrong\" template - e.g. from different filter, and see how it impacts the subtraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf0579",
   "metadata": {},
   "source": [
    "# Zackay, Ofek and Gal-Yam (ZOGY) method of image subtraction (optional)\n",
    "\n",
    "Below I will show how to use one more image subtraction method that is being widely used nowadays. It is often called ZOGY, after the names of its authors, and was first published in [Zackay, Ofek and Gal-Yam (2016)](http://dx.doi.org/10.3847/0004-637X/830/1/27). The method itself does not fit for anything, and thus it requires pre-determined PSFs for both image and template, along with their relative flux scale. Then it both performs the subtraction (basically, by convolving the image with template PSF, and the template - with image PSF, thus matching their resolutions), and provides statistically well-defined estimators for the significance of point source detections in every point of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a76a0dd",
   "metadata": {},
   "source": [
    "To use the method, we will need to get the PSFs. We will use [PSFEx](https://github.com/astromatic/psfex) for it, again through the wrapper, as documented at https://stdpipe.readthedocs.io/en/latest/psf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_psf,image_obj = psf.run_psfex(\n",
    "    image, \n",
    "    mask=mask, \n",
    "    aper=3.0*fwhm, \n",
    "    gain=gain,\n",
    "    get_obj=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Normalized PSF stamp with original image resolution\n",
    "image_psf_stamp = psf.get_psf_stamp(image_psf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3098b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_psf,template_obj = psf.run_psfex(\n",
    "    tmpl, \n",
    "    mask=tmask, \n",
    "    aper=3.0*template_fwhm, \n",
    "    gain=1e6,\n",
    "    get_obj=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Normalized PSF stamp with original template resolution\n",
    "template_psf_stamp = psf.get_psf_stamp(template_psf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c603ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plots.imshow(image_psf_stamp, show_colorbar=False)\n",
    "plt.subplot(122)\n",
    "plots.imshow(template_psf_stamp, show_colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f7acf",
   "metadata": {},
   "source": [
    "Now we have to get the relative flux scale of the images. We will use object lists returned by `run_psfex` above - they contains the fluxes in large apertures, and thus may be used for it (beware of nearby sources though!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use half FWHM radius for matching\n",
    "sr = 0.5*max(image_psf['fwhm'], template_psf['fwhm'])\n",
    "\n",
    "iidx,tidx,_ = astrometry.planar_match(\n",
    "    image_obj['x'], image_obj['y'],\n",
    "    template_obj['x'], template_obj['y'],\n",
    "    sr\n",
    ")\n",
    "\n",
    "# Exclude all flagged objects\n",
    "fidx = (image_obj['flags'][iidx] & 0x100) == 0\n",
    "fidx &= (template_obj['flags'][tidx] & 0x100) == 0\n",
    "\n",
    "if np.sum(fidx) < 5:\n",
    "    print('Not enough matched points for scale estimation!')\n",
    "    \n",
    "# Compute photometric zero point shift between two object lists\n",
    "ZP = template_obj['mag'][tidx][fidx] - image_obj['mag'][iidx][fidx]\n",
    "dZP = np.hypot(image_obj['magerr'][iidx][fidx], template_obj['magerr'][tidx][fidx])\n",
    "\n",
    "# Naive estimation\n",
    "ZP_naive = np.median(ZP)\n",
    "print('Naive ZP is', ZP_naive)\n",
    "\n",
    "# More accurate - weighted robust regression\n",
    "import statsmodels.api as sm\n",
    "X = np.ones_like(ZP).T\n",
    "C = sm.RLM(ZP / dZP, (X.T / dZP).T).fit()\n",
    "ZP_robust = C.params[0]\n",
    "print('Robust ZP is', ZP_naive)\n",
    "\n",
    "scale = 10**(-0.4*ZP_robust)\n",
    "print(\"Estimated initial template flux scale Fr = %.3g\" % scale)\n",
    "\n",
    "plt.errorbar(\n",
    "    image_obj['mag'][iidx][fidx], \n",
    "    ZP,\n",
    "    dZP,\n",
    "    fmt='.'\n",
    ")\n",
    "plt.axhline(ZP_naive, color='red', ls=':', label='Naive zero point')\n",
    "plt.axhline(ZP_robust, color='red', ls='--', label='Robust zero point')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48898",
   "metadata": {},
   "source": [
    "The cell below contains an implementaion of image subtraction algorithm, as described in [ZOGY paper](http://dx.doi.org/10.3847/0004-637X/830/1/27). Equation numbers, variable names and comments directly refer to the paper. Feel free to read it, the paper is really interesting! :-)\n",
    "\n",
    "*STDPipe* also contains a variant of this code, with some additional things (like fitting for flux scale and offset) implemented. The code below should be equivalent to \n",
    "```\n",
    "D, S_corr = subtraction.run_zogy(image - bg, tmpl - template_bg, mask=mask, template_mask=tmask, image_psf=image_psf, template_psf=template_psf, image_gain=gain, template_gain=1e6, scale=scale, fit_scale=False, fit_shift=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2579e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-in replacement for numpy.fft which is supposedly faster\n",
    "# import pyfftw.interfaces.numpy_fft as fft\n",
    "from numpy import fft\n",
    "\n",
    "image_bg = photutils.Background2D(image, 64, mask=mask)\n",
    "template_bg = photutils.Background2D(tmpl, 64, mask=tmask)\n",
    "\n",
    "# Subtract backgrounds\n",
    "N = image - image_bg.background\n",
    "R = tmpl - template_bg.background\n",
    "# Set to zero the regions where we have no usable data\n",
    "N[mask] = 0\n",
    "R[tmask] = 0\n",
    "\n",
    "# Image noise model\n",
    "U_N = image_bg.background_rms\n",
    "U_N = np.sqrt(U_N ** 2 + np.abs(N) / gain)\n",
    "SN = image_bg.background_rms_median\n",
    "\n",
    "# Template noise model\n",
    "U_R = template_bg.background_rms\n",
    "#U_R = np.sqrt(U_R ** 2 + np.abs(R) / template_gain) \n",
    "SR = template_bg.background_rms_median\n",
    "\n",
    "# Artificially assign large uncertainty to the regions we set to zero\n",
    "U_N[mask] = np.nanmax(U_N)\n",
    "U_R[tmask] = np.nanmax(U_R)\n",
    "\n",
    "# PSF stamps\n",
    "P_N_small = psf.get_psf_stamp(image_psf)\n",
    "P_R_small = psf.get_psf_stamp(template_psf)\n",
    "\n",
    "Fn = 1 # Fixed, so that the difference will be in science image flux scale\n",
    "Fr = scale \n",
    "\n",
    "# Expected astrometric errors\n",
    "dx = 0.25\n",
    "dy = 0.25\n",
    "\n",
    "# Place PSF at the center of image with same size as new / reference\n",
    "P_N = np.zeros_like(N)\n",
    "P_R = np.zeros_like(R)\n",
    "idxN = tuple([slice(int(N.shape[0]/2) - int(P_N_small.shape[0]/2),\n",
    "                    int(N.shape[0]/2) + int(P_N_small.shape[0]/2) + 1),\n",
    "              slice(int(N.shape[1]/2) - int(P_N_small.shape[1]/2),\n",
    "                    int(N.shape[1]/2) + int(P_N_small.shape[1]/2) + 1)])\n",
    "idxR = tuple([slice(int(R.shape[0]/2) - int(P_R_small.shape[0]/2),\n",
    "                    int(R.shape[0]/2) + int(P_R_small.shape[0]/2) + 1),\n",
    "              slice(int(R.shape[1]/2) - int(P_R_small.shape[1]/2),\n",
    "                    int(R.shape[1]/2) + int(P_R_small.shape[1]/2) + 1)])\n",
    "P_N[idxN] = P_N_small\n",
    "P_R[idxR] = P_R_small\n",
    "\n",
    "# Shift the PSF to the origin so it will not introduce a shift\n",
    "P_N = fft.fftshift(P_N)\n",
    "P_R = fft.fftshift(P_R)\n",
    "\n",
    "# Take all the Fourier Transforms\n",
    "N_hat = fft.fft2(N)\n",
    "R_hat = fft.fft2(R)\n",
    "\n",
    "P_N_hat = fft.fft2(P_N)\n",
    "P_R_hat = fft.fft2(P_R)\n",
    "\n",
    "# Fourier Transform of Difference Image (Equation 13)\n",
    "D_hat_num = (Fr * P_R_hat * N_hat - Fn * P_N_hat * R_hat)\n",
    "D_hat_den = np.sqrt(SN**2 * Fr**2 * np.abs(P_R_hat**2) + SR**2 * Fn**2 * np.abs(P_N_hat**2))\n",
    "D_hat = D_hat_num / D_hat_den\n",
    "\n",
    "# Flux-based zero point (Equation 15)\n",
    "FD = Fr * Fn / np.sqrt(SN**2 * Fr**2 + SR**2 * Fn**2)\n",
    "\n",
    "# Difference image corrected for correlated noise\n",
    "D = np.real(fft.ifft2(D_hat)) / FD\n",
    "\n",
    "# Fourier Transform of PSF of Subtraction Image (Equation 14)\n",
    "P_D_hat = Fr * Fn * P_R_hat * P_N_hat / FD / D_hat_den\n",
    "\n",
    "# PSF of Subtraction Image D\n",
    "P_D = np.real(fft.ifft2(P_D_hat))\n",
    "P_D = fft.ifftshift(P_D)\n",
    "P_D = P_D[idxN]\n",
    "\n",
    "# Fourier Transform of Score Image (Equation 17)\n",
    "S_hat = FD * D_hat * np.conj(P_D_hat)\n",
    "\n",
    "# Score Image\n",
    "S = np.real(fft.ifft2(S_hat))\n",
    "\n",
    "# Now start calculating Scorr matrix (including all noise terms)\n",
    "\n",
    "# Start out with source noise\n",
    "\n",
    "# Sigma to variance\n",
    "V_N = U_N**2\n",
    "V_R = U_R**2\n",
    "\n",
    "# Fourier Transform of variance images\n",
    "V_N_hat = fft.fft2(V_N)\n",
    "V_R_hat = fft.fft2(V_R)\n",
    "\n",
    "# Equation 28\n",
    "kr_hat = Fr * Fn**2 * np.conj(P_R_hat) * np.abs(P_N_hat**2) / (D_hat_den**2)\n",
    "kr = np.real(fft.ifft2(kr_hat))\n",
    "\n",
    "# Equation 29\n",
    "kn_hat = Fn * Fr**2 * np.conj(P_N_hat) * np.abs(P_R_hat**2) / (D_hat_den**2)\n",
    "kn = np.real(fft.ifft2(kn_hat))\n",
    "\n",
    "# Noise in New Image: Equation 26\n",
    "V_S_N = np.real(fft.ifft2(V_N_hat * fft.fft2(kn**2)))\n",
    "\n",
    "# Noise in Reference Image: Equation 27\n",
    "V_S_R = np.real(fft.ifft2(V_R_hat * fft.fft2(kr**2)))\n",
    "\n",
    "# Astrometric Noise\n",
    "# Equation 31\n",
    "S_N = np.real(fft.ifft2(kn_hat * N_hat))\n",
    "dSNdx = S_N - np.roll(S_N, 1, axis=1)\n",
    "dSNdy = S_N - np.roll(S_N, 1, axis=0)\n",
    "\n",
    "# Equation 30\n",
    "V_ast_S_N = dx**2 * dSNdx**2 + dy**2 * dSNdy**2\n",
    "\n",
    "# Equation 33\n",
    "S_R = np.real(fft.ifft2(kr_hat * R_hat))\n",
    "dSRdx = S_R - np.roll(S_R, 1, axis=1)\n",
    "dSRdy = S_R - np.roll(S_R, 1, axis=0)\n",
    "\n",
    "# Equation 32\n",
    "V_ast_S_R = dx**2 * dSRdx**2 + dy**2 * dSRdy**2\n",
    "\n",
    "# Calculate Scorr\n",
    "S_corr = S / np.sqrt(V_S_N + V_S_R + V_ast_S_N + V_ast_S_R)\n",
    "\n",
    "# PSF photometry (Equations 41-43)\n",
    "F_S = np.sum(Fn**2 * Fr**2 * np.abs(P_N_hat**2) * np.abs(P_R_hat**2) / (D_hat_den**2))\n",
    "F_S /= S.shape[1]*S.shape[1] # divide by the number of pixels due to FFT normalization\n",
    "Fpsf = S / F_S # optimal PSF photometry, alpha in Equation 41\n",
    "Fpsf_err = np.sqrt(V_S_N + V_S_R) / F_S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8956eb",
   "metadata": {},
   "source": [
    "Now we may plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c840dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(D)\n",
    "plt.title('Difference image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.imshow(S_corr, vmin=-3, vmax=10)\n",
    "plt.title('Scorr - PSF-matched significance for the point sources')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f979cc",
   "metadata": {},
   "source": [
    "Implementing transient detection on Scorr image is trivial and is left as an exercise for the reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dd676",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plots.imshow(sdiff, vmin=-3, vmax=10)\n",
    "plt.title('Noise-scaled difference - HOTPANTS')\n",
    "\n",
    "plt.subplot(122)\n",
    "plots.imshow(S_corr, vmin=-3, vmax=10)\n",
    "plt.title('Scorr - ZOGY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945241c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STDPipe",
   "language": "python",
   "name": "stdpipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
